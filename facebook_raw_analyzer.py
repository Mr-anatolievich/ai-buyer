#!/usr/bin/env python3
"""
Facebook Raw Data Analyzer
–ó–±–µ—Ä—ñ–≥–∞—î —Å–∏—Ä—ñ HTML –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
"""

import json
import sqlite3
import time
import urllib.request
import gzip
import os

def save_raw_facebook_data():
    """–ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å–∏—Ä—ñ –¥–∞–Ω—ñ Facebook –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –∑ –±–∞–∑–∏
    conn = sqlite3.connect('ai_buyer.db')
    cursor = conn.cursor()
    cursor.execute("""
        SELECT facebook_id, cookies_data, user_agent 
        FROM facebook_accounts WHERE facebook_id = '100009868933766'
    """)
    
    account_data = cursor.fetchone()
    conn.close()
    
    if not account_data:
        print("‚ùå –ê–∫–∞—É–Ω—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∏–π")
        return
    
    facebook_id, cookies_data, user_agent = account_data
    
    headers = {
        'User-Agent': user_agent,
        'Cookie': cookies_data,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive'
    }
    
    # URL –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    urls = {
        'ads_manager_campaigns': 'https://www.facebook.com/adsmanager/manage/campaigns',
        'ads_manager_accounts': 'https://www.facebook.com/adsmanager/manage/adaccounts',
        'business_ads_manager': 'https://business.facebook.com/adsmanager/',
        'facebook_main': 'https://www.facebook.com/'
    }
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    os.makedirs('facebook_raw_data', exist_ok=True)
    
    results = {}
    
    for name, url in urls.items():
        print(f"üì° –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ: {name} ({url})")
        
        try:
            request = urllib.request.Request(url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=15) as response:
                content = response.read()
                
                # –†–æ–∑–ø–∞–∫–æ–≤—É—î–º–æ gzip
                if response.headers.get('Content-Encoding') == 'gzip':
                    content = gzip.decompress(content)
                
                # –î–µ–∫–æ–¥—É—î–º–æ
                try:
                    html = content.decode('utf-8')
                except UnicodeDecodeError:
                    html = content.decode('utf-8', errors='ignore')
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª
                filename = f'facebook_raw_data/{name}.html'
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(html)
                
                print(f"   ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {filename} ({len(html):,} —Å–∏–º–≤–æ–ª—ñ–≤)")
                
                # –ë–∞–∑–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑
                analysis = {
                    'url': url,
                    'status': response.status,
                    'size': len(html),
                    'filename': filename,
                    'contains_act_': 'act_' in html,
                    'contains_adaccount': 'adaccount' in html.lower(),
                    'contains_campaign': 'campaign' in html.lower(),
                    'contains_dtsg': 'DTSGInitData' in html,
                    'contains_requirejs': 'requireLazy' in html,
                    'contains_graphql': 'graphql' in html.lower()
                }
                
                results[name] = analysis
                
                # –ü–æ—à—É–∫ –≤—Å—ñ—Ö —á–∏—Å–ª–æ–≤–∏—Ö ID (–ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω—ñ account ID)
                import re
                numeric_ids = re.findall(r'\b\d{8,20}\b', html)
                unique_ids = list(set(numeric_ids))
                
                if len(unique_ids) > 0:
                    print(f"   üî¢ –ó–Ω–∞–π–¥–µ–Ω–æ —á–∏—Å–ª–æ–≤–∏—Ö ID: {len(unique_ids)}")
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–µ—Ä—à—ñ 20 –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
                    analysis['numeric_ids'] = unique_ids[:20]
                
                print()
                
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            results[name] = {'error': str(e)}
        
        time.sleep(2)  # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑
    with open('facebook_raw_data/analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("üìä –ü–Ü–î–°–£–ú–û–ö –ê–ù–ê–õ–Ü–ó–£:")
    for name, data in results.items():
        if 'error' not in data:
            print(f"\n{name}:")
            print(f"   –†–æ–∑–º—ñ—Ä: {data['size']:,} —Å–∏–º–≤–æ–ª—ñ–≤")
            print(f"   –ú—ñ—Å—Ç–∏—Ç—å 'act_': {data['contains_act_']}")
            print(f"   –ú—ñ—Å—Ç–∏—Ç—å 'adaccount': {data['contains_adaccount']}")
            print(f"   –ú—ñ—Å—Ç–∏—Ç—å 'campaign': {data['contains_campaign']}")
            print(f"   –ú—ñ—Å—Ç–∏—Ç—å DTSG: {data['contains_dtsg']}")
            print(f"   –ú—ñ—Å—Ç–∏—Ç—å RequireJS: {data['contains_requirejs']}")
            print(f"   –ú—ñ—Å—Ç–∏—Ç—å GraphQL: {data['contains_graphql']}")
            
            if 'numeric_ids' in data:
                print(f"   –ß–∏—Å–ª–æ–≤—ñ ID: {data['numeric_ids'][:5]}...")


def analyze_saved_files():
    """–ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —Ñ–∞–π–ª–∏ –±—ñ–ª—å—à –¥–µ—Ç–∞–ª—å–Ω–æ"""
    
    if not os.path.exists('facebook_raw_data'):
        print("‚ùå –ü–∞–ø–∫–∞ facebook_raw_data –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å save_raw_facebook_data()")
        return
    
    print("üîç –î–ï–¢–ê–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–ó –ó–ë–ï–†–ï–ñ–ï–ù–ò–• –§–ê–ô–õ–Ü–í:")
    
    for filename in os.listdir('facebook_raw_data'):
        if filename.endswith('.html'):
            filepath = os.path.join('facebook_raw_data', filename)
            
            print(f"\nüìÑ –ê–Ω–∞–ª—ñ–∑—É—î–º–æ: {filename}")
            
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –î–µ—Ç–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–∞—Ç–µ—Ä–Ω—ñ–≤
            import re
            
            patterns = {
                'Facebook IDs (100...)': r'\b100\d{12,15}\b',
                'Ad Account IDs (act_)': r'act_\d+',  
                'Account IDs (—á–∏—Å–ª–æ–≤—ñ)': r'"account_id":\s*"?(\d+)"?',
                'Entity IDs': r'"entityID":\s*"?(\d+)"?',
                'Page IDs': r'"pageID":\s*"?(\d+)"?',
                'Business IDs': r'"businessID":\s*"?(\d+)"?',
                'DTSG Tokens': r'"DTSGInitData"[^"]*"token":\s*"([^"]{10,})"',
                'API Endpoints': r'"/api/[^"]*"',
                'GraphQL Doc IDs': r'"doc_id":\s*"?(\d+)"?',
                'RequireJS Modules': r'requireLazy\(\["([^"]*Ad[^"]*)"'
            }
            
            for pattern_name, pattern in patterns.items():
                matches = re.findall(pattern, content, re.IGNORECASE)
                unique_matches = list(set(matches))
                
                if unique_matches:
                    print(f"   {pattern_name}: {len(unique_matches)} –∑–Ω–∞–π–¥–µ–Ω–æ")
                    # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ –∫—ñ–ª—å–∫–∞ –∑–±—ñ–≥—ñ–≤
                    for match in unique_matches[:3]:
                        if len(str(match)) > 50:
                            print(f"     - {str(match)[:47]}...")
                        else:
                            print(f"     - {match}")
                    if len(unique_matches) > 3:
                        print(f"     ... —ñ —â–µ {len(unique_matches) - 3}")


if __name__ == '__main__':
    print("üîç Facebook Raw Data Analyzer")
    print("1Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–∏—Ä—ñ –¥–∞–Ω—ñ...")
    save_raw_facebook_data()
    
    print("\n2Ô∏è‚É£ –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —Ñ–∞–π–ª–∏...")
    analyze_saved_files()
    
    print(f"\nüìÅ –î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –ø–∞–ø—Ü—ñ: facebook_raw_data/")
    print("üí° –ú–æ–∂–µ—Ç–µ –≤—ñ–¥–∫—Ä–∏—Ç–∏ HTML —Ñ–∞–π–ª–∏ –≤ –±—Ä–∞—É–∑–µ—Ä—ñ –¥–ª—è –≤—ñ–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É")
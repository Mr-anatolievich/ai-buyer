#!/usr/bin/env python3
"""
Facebook XHR Enhanced Pattern Extractor
–í–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–∏–π –ø–æ—à—É–∫ –¥–∞–Ω–∏—Ö –≤ Facebook HTML –∑ –∫—Ä–∞—â–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
"""

import json
import sqlite3
import time
import urllib.request
import urllib.parse
import gzip
import re
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse

class FacebookEnhancedExtractor:
    """–í–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–∏–π –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∑ –∫—Ä–∞—â–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
    
    def __init__(self, cookies_data, user_agent, facebook_id):
        self.cookies_data = cookies_data
        self.user_agent = user_agent
        self.facebook_id = facebook_id
        
    def get_base_headers(self):
        """–ë–∞–∑–æ–≤—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏"""
        return {
            'User-Agent': self.user_agent,
            'Cookie': self.cookies_data,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,uk;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Cache-Control': 'max-age=0'
        }
    
    def make_request(self, url, timeout=15):
        """HTTP –∑–∞–ø–∏—Ç –∑ –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫"""
        try:
            print(f"üì° –ó–∞–ø–∏—Ç: {url}")
            
            request = urllib.request.Request(url, headers=self.get_base_headers())
            
            with urllib.request.urlopen(request, timeout=timeout) as response:
                content = response.read()
                
                # –†–æ–∑–ø–∞–∫–æ–≤—É—î–º–æ gzip
                if response.headers.get('Content-Encoding') == 'gzip':
                    content = gzip.decompress(content)
                
                # –î–µ–∫–æ–¥—É—î–º–æ —Ç–µ–∫—Å—Ç
                try:
                    text = content.decode('utf-8')
                except UnicodeDecodeError:
                    text = content.decode('utf-8', errors='ignore')
                
                print(f"   ‚úÖ Status: {response.status}, Length: {len(text):,}")
                return text
                
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            return None
    
    def enhanced_pattern_search(self, html_content, url):
        """–í–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–∏–π –ø–æ—à—É–∫ –ø–∞—Ç–µ—Ä–Ω—ñ–≤ –≤ HTML"""
        print(f"\nüîç –ê–Ω–∞–ª—ñ–∑—É—î–º–æ HTML –∑ {url}")
        print(f"   –†–æ–∑–º—ñ—Ä: {len(html_content):,} —Å–∏–º–≤–æ–ª—ñ–≤")
        
        findings = []
        
        # 1. –ü–æ—à—É–∫ JavaScript –æ–±'—î–∫—Ç—ñ–≤ –∑ —Ä–µ–∫–ª–∞–º–Ω–∏–º–∏ –∫–∞–±—ñ–Ω–µ—Ç–∞–º–∏
        js_patterns = [
            r'"adAccountId":\s*"(\d+)"',
            r'"account_id":\s*"(\d+)"', 
            r'"accountId":\s*"(\d+)"',
            r'"adaccount_id":\s*"(\d+)"',
            r'"id":\s*"act_(\d+)"',
            r'act_(\d+)',
            r'"entityID":\s*"(\d+)"',
            r'"advertiserAccountID":\s*"(\d+)"'
        ]
        
        account_ids = set()
        
        for pattern in js_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    account_id = match[0] if match[0] else match[1]
                else:
                    account_id = match
                
                if account_id and account_id.isdigit() and len(account_id) > 5:
                    account_ids.add(account_id)
        
        if account_ids:
            print(f"   ‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ ID –∫–∞–±—ñ–Ω–µ—Ç—ñ–≤: {list(account_ids)}")
            for acc_id in account_ids:
                findings.append({
                    'type': 'ad_account_id',
                    'id': f'act_{acc_id}',
                    'account_id': acc_id,
                    'source': 'regex_pattern',
                    'url': url
                })
        
        # 2. –ü–æ—à—É–∫ RequireJS –º–æ–¥—É–ª—ñ–≤
        require_pattern = r'requireLazy\(\["([^"]*[Aa]d[^"]*)"[^\]]*\]'
        require_matches = re.findall(require_pattern, html_content)
        
        if require_matches:
            print(f"   üì¶ RequireJS –º–æ–¥—É–ª—ñ: {require_matches}")
            findings.append({
                'type': 'requirejs_modules',
                'modules': require_matches,
                'source': 'requirejs',
                'url': url
            })
        
        # 3. –ü–æ—à—É–∫ GraphQL –∑–∞–ø–∏—Ç—ñ–≤
        graphql_patterns = [
            r'"doc_id":\s*"?(\d+)"?',
            r'"documentID":\s*"?(\d+)"?',
            r'"queryID":\s*"?(\d+)"?'
        ]
        
        for pattern in graphql_patterns:
            matches = re.findall(pattern, html_content)
            if matches:
                print(f"   üîó GraphQL –¥–æ–∫—É–º–µ–Ω—Ç–∏: {matches}")
                findings.append({
                    'type': 'graphql_doc_ids',
                    'doc_ids': matches,
                    'source': 'graphql_pattern',
                    'url': url
                })
        
        # 4. –ü–æ—à—É–∫ DTSG —Ç–æ–∫–µ–Ω—ñ–≤
        dtsg_pattern = r'"DTSGInitData"[^"]*"token":\s*"([^"]+)"'
        dtsg_matches = re.findall(dtsg_pattern, html_content)
        
        if dtsg_matches:
            print(f"   üîê DTSG —Ç–æ–∫–µ–Ω–∏: {len(dtsg_matches)} –∑–Ω–∞–π–¥–µ–Ω–æ")
            findings.append({
                'type': 'dtsg_tokens',
                'tokens': dtsg_matches,
                'source': 'dtsg_pattern',
                'url': url
            })
        
        # 5. –ü–æ—à—É–∫ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó Ads Manager
        config_patterns = [
            r'"adsManagerConfig":\s*(\{[^}]+\})',
            r'"adAccountsData":\s*(\[[^\]]+\])',
            r'"campaignData":\s*(\{[^}]+\})'
        ]
        
        for pattern in config_patterns:
            matches = re.findall(pattern, html_content)
            if matches:
                print(f"   ‚öôÔ∏è –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–Ω–∞–π–¥–µ–Ω–∞: {len(matches)} –∑–±—ñ–≥—ñ–≤")
                for match in matches:
                    try:
                        config_data = json.loads(match)
                        findings.append({
                            'type': 'ads_manager_config',
                            'config': config_data,
                            'source': 'config_pattern',
                            'url': url
                        })
                    except json.JSONDecodeError:
                        continue
        
        # 6. –ü–æ—à—É–∫ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —Ñ–æ—Ä–º –∑ —Ç–æ–∫–µ–Ω–∞–º–∏
        form_pattern = r'<input[^>]*name="([^"]*token[^"]*)"[^>]*value="([^"]*)"'
        form_matches = re.findall(form_pattern, html_content, re.IGNORECASE)
        
        if form_matches:
            print(f"   üìù –§–æ—Ä–º–∏ –∑ —Ç–æ–∫–µ–Ω–∞–º–∏: {len(form_matches)}")
            findings.append({
                'type': 'form_tokens',
                'tokens': dict(form_matches),
                'source': 'form_pattern',
                'url': url
            })
        
        # 7. –ü–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ —ñ–Ω—à—ñ API –µ–Ω–¥–ø–æ—ñ–Ω—Ç–∏
        api_pattern = r'"(/api/[^"]+)"'
        api_matches = re.findall(api_pattern, html_content)
        
        if api_matches:
            unique_apis = list(set(api_matches))
            print(f"   üîó API –µ–Ω–¥–ø–æ—ñ–Ω—Ç–∏: {len(unique_apis)}")
            findings.append({
                'type': 'api_endpoints',
                'endpoints': unique_apis,
                'source': 'api_pattern',
                'url': url
            })
        
        return findings
    
    def extract_comprehensive_data(self):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö"""
        print("üöÄ –ó–∞–ø—É—Å–∫–∞—î–º–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è...")
        
        # –°–ø–∏—Å–æ–∫ URL –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        urls_to_analyze = [
            'https://www.facebook.com/adsmanager/manage/campaigns',
            'https://www.facebook.com/adsmanager/manage/adaccounts', 
            'https://business.facebook.com/adsmanager/',
            'https://www.facebook.com/adsmanager/',
            'https://www.facebook.com/'  # –ì–æ–ª–æ–≤–Ω–∞ —è–∫ fallback
        ]
        
        all_findings = []
        total_accounts = set()
        
        for url in urls_to_analyze:
            html_content = self.make_request(url)
            
            if html_content:
                findings = self.enhanced_pattern_search(html_content, url)
                all_findings.extend(findings)
                
                # –ó–±–∏—Ä–∞—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ ID –∫–∞–±—ñ–Ω–µ—Ç—ñ–≤
                for finding in findings:
                    if finding['type'] == 'ad_account_id':
                        total_accounts.add(finding['account_id'])
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
            time.sleep(2)
        
        # –ü—ñ–¥—Å—É–º–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        result = {
            'facebook_id': self.facebook_id,
            'method': 'enhanced_pattern_extraction',
            'total_accounts_found': len(total_accounts),
            'account_ids': list(total_accounts),
            'detailed_findings': all_findings,
            'urls_analyzed': len(urls_to_analyze),
            'timestamp': time.time()
        }
        
        print(f"\nüìä –ü—ñ–¥—Å—É–º–æ–∫ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è:")
        print(f"   –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ URL: {len(urls_to_analyze)}")
        print(f"   –ó–Ω–∞–π–¥–µ–Ω–æ –∫–∞–±—ñ–Ω–µ—Ç—ñ–≤: {len(total_accounts)}")
        print(f"   –í—Å—å–æ–≥–æ –∑–Ω–∞—Ö—ñ–¥–æ–∫: {len(all_findings)}")
        
        if total_accounts:
            print(f"   ID –∫–∞–±—ñ–Ω–µ—Ç—ñ–≤: {list(total_accounts)}")
        
        return result


# –ü—Ä–æ—Å—Ç–∏–π HTTP —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
class EnhancedExtractorHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/':
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            
            response = {
                'service': 'Facebook Enhanced Pattern Extractor',
                'version': '2.0',
                'status': 'active'
            }
            self.wfile.write(json.dumps(response, indent=2).encode())
        
        elif self.path.startswith('/extract/'):
            account_id = self.path.split('/')[-1]
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –∑ –±–∞–∑–∏
            try:
                conn = sqlite3.connect('ai_buyer.db')
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT facebook_id, cookies_data, user_agent 
                    FROM facebook_accounts WHERE id = ?
                """, (account_id,))
                
                account_data = cursor.fetchone()
                conn.close()
                
                if account_data:
                    facebook_id, cookies_data, user_agent = account_data
                    
                    extractor = FacebookEnhancedExtractor(
                        cookies_data=cookies_data,
                        user_agent=user_agent, 
                        facebook_id=facebook_id
                    )
                    
                    result = extractor.extract_comprehensive_data()
                    
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    self.wfile.write(json.dumps(result, ensure_ascii=False, indent=2).encode())
                else:
                    self.send_error(404, 'Account not found')
                    
            except Exception as e:
                self.send_error(500, str(e))
        else:
            self.send_error(404)


def main():
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ –¥–∞–Ω–∏–º–∏ –∑ –±–∞–∑–∏
    try:
        conn = sqlite3.connect('ai_buyer.db')
        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, facebook_id, cookies_data, user_agent 
            FROM facebook_accounts 
            WHERE facebook_id = '100009868933766'
        """)
        
        account_data = cursor.fetchone()
        conn.close()
        
        if account_data:
            account_id, facebook_id, cookies_data, user_agent = account_data
            
            print("üéØ –¢–µ—Å—Ç—É—î–º–æ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è...")
            print(f"   Account ID: {account_id}")
            print(f"   Facebook ID: {facebook_id}")
            print(f"   Cookies: {'–Ñ' if cookies_data else '–í—ñ–¥—Å—É—Ç–Ω—ñ'}")
            print(f"   User Agent: {'–Ñ' if user_agent else '–í—ñ–¥—Å—É—Ç–Ω—ñ–π'}")
            
            extractor = FacebookEnhancedExtractor(
                cookies_data=cookies_data,
                user_agent=user_agent,
                facebook_id=facebook_id
            )
            
            result = extractor.extract_comprehensive_data()
            
            # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            print(f"\nüéâ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
            
        else:
            print("‚ùå –ê–∫–∞—É–Ω—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∏–π –≤ –±–∞–∑—ñ")
            
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")


if __name__ == '__main__':
    main()